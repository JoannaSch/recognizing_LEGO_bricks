{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import cv2\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 720000 images belonging to 200 classes.\n",
      "Found 80000 images belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "    validation_split=0.1,\n",
    "    rescale=1./255\n",
    ")\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    class_mode='categorical',\n",
    "    target_size=(64, 64),\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    batch_size=256,\n",
    "    seed=0\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    class_mode='categorical',\n",
    "    target_size=(64, 64),\n",
    "    subset='validation',\n",
    "    shuffle=True,\n",
    "    batch_size=256,\n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(inShape, convNum, hNum, hAct, outNum, loss, drop_rate):\n",
    "    \"\"\"\n",
    "    :param inShape: kształt wejścia, h*w*c\n",
    "    :param convNum: lista ilości featerów w warstwach konwolucyjnych\n",
    "    :param hNum: lista ilości neuronów w poszczególnych warstwach ukrytych\n",
    "    :param hAct: rodzaj aktywacji w warstwach ukrytych, str\n",
    "    :param outNum: ilość\n",
    "    :param loss: funkcja kosztu\n",
    "    \"\"\"\n",
    "    # pusty model\n",
    "    model = tf.keras.Sequential()\n",
    "    # warstwa wejściowa\n",
    "    model.add(layers.InputLayer(input_shape=(inShape)))\n",
    "    # warstwy konwolucyjne\n",
    "    for idx, oneConvNum in enumerate(convNum):\n",
    "        # dodajemy kolejną warstwę konwolucyjną\n",
    "        # zmienia nam ilość cech(kanałów) na oneConvNum\n",
    "        model.add(layers.Conv2D(\n",
    "                                    filters=oneConvNum,\n",
    "                                    kernel_size=3,\n",
    "                                    strides=1,\n",
    "                                    padding=\"SAME\",\n",
    "                                    activation=\"relu\"\n",
    "                                    )\n",
    "                                )\n",
    "        # doajemy następującego po convie maxpool'a\n",
    "        # zmniejsza nam wymiarowość HxW o połowę\n",
    "        model.add(layers.MaxPool2D(\n",
    "                                   pool_size=(2, 2),\n",
    "                                    strides=2,\n",
    "                                   padding=\"SAME\"\n",
    "                                    )\n",
    "                                )\n",
    "        #model.add(layers.Dropout(drop_rate))\n",
    "    # spłaszczamy po konwolucjach, przygotowanie pod relu\n",
    "    model.add(layers.Flatten())\n",
    "    # warstwy fully connected\n",
    "    for idx, oneHidNum in enumerate(hNum):\n",
    "        # dodajemy warstwę\n",
    "        model.add(layers.Dense(oneHidNum, activation=hAct))\n",
    "        model.add(layers.Dropout(drop_rate))\n",
    "    # warstwa wyjściowa\n",
    "    model.add(layers.Dense(outNum))\n",
    "    # warstwa softmax, dystrybucja prawdopodobieństwa\n",
    "    model.add(layers.Softmax())\n",
    "    print(model.summary())\n",
    "    \n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0015)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=opt,\n",
    "        loss=loss,\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 80000 images belonging to 200 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    class_mode='categorical',\n",
    "    target_size=(64, 64),\n",
    "    subset='validation',\n",
    "    shuffle=True,\n",
    "    batch_size=50000,\n",
    "    seed=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = next(test_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_12 (Conv2D)          (None, 64, 64, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d_12 (MaxPoolin  (None, 32, 32, 32)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 32, 32, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_13 (MaxPoolin  (None, 16, 16, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 16, 16, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPoolin  (None, 8, 8, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 8192)              0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 256)               2097408   \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_9 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 256)               65792     \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 200)               51400     \n",
      "                                                                 \n",
      " softmax_4 (Softmax)         (None, 200)               0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,373,640\n",
      "Trainable params: 2,373,640\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "modelCNN3 = create_cnn_model(\n",
    "    inShape=(64,64,3),\n",
    "    convNum=[32,64,128],\n",
    "    hNum=[256, 256, 256],\n",
    "    hAct=\"relu\",\n",
    "    outNum=200,\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    drop_rate = 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dodajemy zapisywanie logów do tensorboard'a\n",
    "log_dir = \"logs\\\\\" + \"norm\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\" + \"diff_convNum_dropout_0_larger_filters_3_hidden_256_neurons\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "50/50 [==============================] - 45s 667ms/step - loss: 5.2913 - accuracy: 0.0055 - val_loss: 5.2610 - val_accuracy: 0.0078\n",
      "Epoch 2/150\n",
      "50/50 [==============================] - 37s 739ms/step - loss: 5.1760 - accuracy: 0.0107 - val_loss: 5.0364 - val_accuracy: 0.0156\n",
      "Epoch 3/150\n",
      "50/50 [==============================] - 36s 707ms/step - loss: 4.8795 - accuracy: 0.0195 - val_loss: 4.7736 - val_accuracy: 0.0195\n",
      "Epoch 4/150\n",
      "50/50 [==============================] - 33s 656ms/step - loss: 4.5828 - accuracy: 0.0373 - val_loss: 4.3879 - val_accuracy: 0.0703\n",
      "Epoch 5/150\n",
      "50/50 [==============================] - 35s 706ms/step - loss: 4.2294 - accuracy: 0.0680 - val_loss: 3.8897 - val_accuracy: 0.1250\n",
      "Epoch 6/150\n",
      "50/50 [==============================] - 35s 696ms/step - loss: 3.9313 - accuracy: 0.0950 - val_loss: 3.7129 - val_accuracy: 0.0898\n",
      "Epoch 7/150\n",
      "50/50 [==============================] - 34s 684ms/step - loss: 3.7376 - accuracy: 0.1195 - val_loss: 3.6786 - val_accuracy: 0.1289\n",
      "Epoch 8/150\n",
      "50/50 [==============================] - 33s 651ms/step - loss: 3.6104 - accuracy: 0.1323 - val_loss: 3.4291 - val_accuracy: 0.1406\n",
      "Epoch 9/150\n",
      "50/50 [==============================] - 33s 654ms/step - loss: 3.5296 - accuracy: 0.1402 - val_loss: 3.3517 - val_accuracy: 0.1641\n",
      "Epoch 10/150\n",
      "50/50 [==============================] - 32s 643ms/step - loss: 3.3617 - accuracy: 0.1656 - val_loss: 3.3282 - val_accuracy: 0.1445\n",
      "Epoch 11/150\n",
      "50/50 [==============================] - 32s 643ms/step - loss: 3.2692 - accuracy: 0.1766 - val_loss: 3.2714 - val_accuracy: 0.1680\n",
      "Epoch 12/150\n",
      "50/50 [==============================] - 33s 646ms/step - loss: 3.1555 - accuracy: 0.1963 - val_loss: 3.1779 - val_accuracy: 0.1875\n",
      "Epoch 13/150\n",
      "50/50 [==============================] - 33s 647ms/step - loss: 3.0789 - accuracy: 0.2099 - val_loss: 2.8788 - val_accuracy: 0.2266\n",
      "Epoch 14/150\n",
      "50/50 [==============================] - 37s 737ms/step - loss: 2.9855 - accuracy: 0.2233 - val_loss: 2.7728 - val_accuracy: 0.2539\n",
      "Epoch 15/150\n",
      "50/50 [==============================] - 34s 674ms/step - loss: 2.9339 - accuracy: 0.2336 - val_loss: 2.6937 - val_accuracy: 0.2695\n",
      "Epoch 16/150\n",
      "50/50 [==============================] - 34s 669ms/step - loss: 2.7934 - accuracy: 0.2551 - val_loss: 2.7764 - val_accuracy: 0.2891\n",
      "Epoch 17/150\n",
      "50/50 [==============================] - 34s 682ms/step - loss: 2.6933 - accuracy: 0.2813 - val_loss: 2.4979 - val_accuracy: 0.3086\n",
      "Epoch 18/150\n",
      "50/50 [==============================] - 34s 675ms/step - loss: 2.6158 - accuracy: 0.2979 - val_loss: 2.4000 - val_accuracy: 0.3477\n",
      "Epoch 19/150\n",
      "50/50 [==============================] - 34s 680ms/step - loss: 2.5638 - accuracy: 0.3094 - val_loss: 2.5114 - val_accuracy: 0.3281\n",
      "Epoch 20/150\n",
      "50/50 [==============================] - 35s 700ms/step - loss: 2.4992 - accuracy: 0.3223 - val_loss: 2.5104 - val_accuracy: 0.3281\n",
      "Epoch 21/150\n",
      "50/50 [==============================] - 36s 718ms/step - loss: 2.4508 - accuracy: 0.3319 - val_loss: 2.5591 - val_accuracy: 0.3125\n",
      "Epoch 22/150\n",
      "50/50 [==============================] - 35s 707ms/step - loss: 2.3920 - accuracy: 0.3455 - val_loss: 2.2789 - val_accuracy: 0.3867\n",
      "Epoch 23/150\n",
      "50/50 [==============================] - 35s 690ms/step - loss: 2.3479 - accuracy: 0.3524 - val_loss: 2.3180 - val_accuracy: 0.3594\n",
      "Epoch 24/150\n",
      "50/50 [==============================] - 34s 685ms/step - loss: 2.3108 - accuracy: 0.3623 - val_loss: 2.2045 - val_accuracy: 0.3906\n",
      "Epoch 25/150\n",
      "50/50 [==============================] - 34s 668ms/step - loss: 2.2591 - accuracy: 0.3738 - val_loss: 2.1507 - val_accuracy: 0.3906\n",
      "Epoch 26/150\n",
      "50/50 [==============================] - 34s 679ms/step - loss: 2.2279 - accuracy: 0.3815 - val_loss: 2.1500 - val_accuracy: 0.4102\n",
      "Epoch 27/150\n",
      "50/50 [==============================] - 35s 690ms/step - loss: 2.1897 - accuracy: 0.3891 - val_loss: 1.9740 - val_accuracy: 0.4062\n",
      "Epoch 28/150\n",
      "50/50 [==============================] - 33s 658ms/step - loss: 2.1599 - accuracy: 0.3899 - val_loss: 2.0336 - val_accuracy: 0.4297\n",
      "Epoch 29/150\n",
      "50/50 [==============================] - 33s 651ms/step - loss: 2.1110 - accuracy: 0.3966 - val_loss: 2.0516 - val_accuracy: 0.3750\n",
      "Epoch 30/150\n",
      "50/50 [==============================] - 33s 657ms/step - loss: 2.0706 - accuracy: 0.4158 - val_loss: 1.8707 - val_accuracy: 0.4766\n",
      "Epoch 31/150\n",
      "50/50 [==============================] - 33s 664ms/step - loss: 2.0569 - accuracy: 0.4173 - val_loss: 1.8789 - val_accuracy: 0.4453\n",
      "Epoch 32/150\n",
      "50/50 [==============================] - 33s 666ms/step - loss: 2.0322 - accuracy: 0.4288 - val_loss: 1.9782 - val_accuracy: 0.4609\n",
      "Epoch 33/150\n",
      "50/50 [==============================] - 33s 651ms/step - loss: 2.0022 - accuracy: 0.4349 - val_loss: 1.9356 - val_accuracy: 0.4922\n",
      "Epoch 34/150\n",
      "50/50 [==============================] - 33s 656ms/step - loss: 1.9647 - accuracy: 0.4353 - val_loss: 1.9223 - val_accuracy: 0.4375\n",
      "Epoch 35/150\n",
      "50/50 [==============================] - 33s 650ms/step - loss: 1.9434 - accuracy: 0.4462 - val_loss: 1.9387 - val_accuracy: 0.4062\n",
      "Epoch 36/150\n",
      "50/50 [==============================] - 36s 712ms/step - loss: 1.9078 - accuracy: 0.4506 - val_loss: 1.9957 - val_accuracy: 0.4375\n",
      "Epoch 37/150\n",
      "50/50 [==============================] - 34s 683ms/step - loss: 1.9113 - accuracy: 0.4529 - val_loss: 1.8036 - val_accuracy: 0.4492\n",
      "Epoch 38/150\n",
      "50/50 [==============================] - 32s 639ms/step - loss: 1.8280 - accuracy: 0.4730 - val_loss: 1.8386 - val_accuracy: 0.4688\n",
      "Epoch 39/150\n",
      "50/50 [==============================] - 36s 722ms/step - loss: 1.8361 - accuracy: 0.4655 - val_loss: 1.9100 - val_accuracy: 0.4375\n",
      "Epoch 40/150\n",
      "50/50 [==============================] - 34s 669ms/step - loss: 1.8481 - accuracy: 0.4652 - val_loss: 1.7312 - val_accuracy: 0.4961\n",
      "Epoch 41/150\n",
      "50/50 [==============================] - 35s 696ms/step - loss: 1.8038 - accuracy: 0.4743 - val_loss: 1.7961 - val_accuracy: 0.5000\n",
      "Epoch 42/150\n",
      "50/50 [==============================] - 33s 651ms/step - loss: 1.7705 - accuracy: 0.4857 - val_loss: 1.5447 - val_accuracy: 0.5234\n",
      "Epoch 43/150\n",
      "50/50 [==============================] - 36s 728ms/step - loss: 1.7575 - accuracy: 0.4862 - val_loss: 1.4812 - val_accuracy: 0.5430\n",
      "Epoch 44/150\n",
      "50/50 [==============================] - 35s 694ms/step - loss: 1.7766 - accuracy: 0.4844 - val_loss: 1.6596 - val_accuracy: 0.4688\n",
      "Epoch 45/150\n",
      "50/50 [==============================] - 35s 689ms/step - loss: 1.7143 - accuracy: 0.4974 - val_loss: 1.6436 - val_accuracy: 0.4805\n",
      "Epoch 46/150\n",
      "50/50 [==============================] - 35s 690ms/step - loss: 1.6879 - accuracy: 0.5071 - val_loss: 1.6830 - val_accuracy: 0.4805\n",
      "Epoch 47/150\n",
      "50/50 [==============================] - 33s 655ms/step - loss: 1.6949 - accuracy: 0.5004 - val_loss: 1.5577 - val_accuracy: 0.5000\n",
      "Epoch 48/150\n",
      "50/50 [==============================] - 33s 663ms/step - loss: 1.6671 - accuracy: 0.5139 - val_loss: 1.5163 - val_accuracy: 0.5352\n",
      "Epoch 49/150\n",
      "50/50 [==============================] - 34s 669ms/step - loss: 1.6754 - accuracy: 0.5096 - val_loss: 1.4847 - val_accuracy: 0.5664\n",
      "Epoch 50/150\n",
      "50/50 [==============================] - 33s 662ms/step - loss: 1.6444 - accuracy: 0.5159 - val_loss: 1.7892 - val_accuracy: 0.5195\n",
      "Epoch 51/150\n",
      "50/50 [==============================] - 34s 669ms/step - loss: 1.5995 - accuracy: 0.5266 - val_loss: 1.5767 - val_accuracy: 0.5586\n",
      "Epoch 52/150\n",
      "50/50 [==============================] - 34s 682ms/step - loss: 1.6129 - accuracy: 0.5208 - val_loss: 1.6467 - val_accuracy: 0.4805\n",
      "Epoch 53/150\n",
      "50/50 [==============================] - 34s 683ms/step - loss: 1.5968 - accuracy: 0.5265 - val_loss: 1.5329 - val_accuracy: 0.5469\n",
      "Epoch 54/150\n",
      "50/50 [==============================] - 34s 673ms/step - loss: 1.6076 - accuracy: 0.5302 - val_loss: 1.5288 - val_accuracy: 0.5078\n",
      "Epoch 55/150\n",
      "50/50 [==============================] - 34s 671ms/step - loss: 1.5604 - accuracy: 0.5349 - val_loss: 1.4625 - val_accuracy: 0.5430\n",
      "Epoch 56/150\n",
      "50/50 [==============================] - 34s 667ms/step - loss: 1.5442 - accuracy: 0.5420 - val_loss: 1.3322 - val_accuracy: 0.6016\n",
      "Epoch 57/150\n",
      "50/50 [==============================] - 33s 655ms/step - loss: 1.5194 - accuracy: 0.5488 - val_loss: 1.3808 - val_accuracy: 0.5938\n",
      "Epoch 58/150\n",
      "50/50 [==============================] - 35s 694ms/step - loss: 1.5871 - accuracy: 0.5326 - val_loss: 1.6329 - val_accuracy: 0.5195\n",
      "Epoch 59/150\n",
      "50/50 [==============================] - 34s 681ms/step - loss: 1.5090 - accuracy: 0.5471 - val_loss: 1.4155 - val_accuracy: 0.5820\n",
      "Epoch 60/150\n",
      "50/50 [==============================] - 34s 679ms/step - loss: 1.4900 - accuracy: 0.5559 - val_loss: 1.4669 - val_accuracy: 0.5469\n",
      "Epoch 61/150\n",
      "50/50 [==============================] - 34s 681ms/step - loss: 1.4765 - accuracy: 0.5591 - val_loss: 1.5449 - val_accuracy: 0.5312\n",
      "Epoch 62/150\n",
      "50/50 [==============================] - 34s 675ms/step - loss: 1.4917 - accuracy: 0.5570 - val_loss: 1.4770 - val_accuracy: 0.5352\n",
      "Epoch 63/150\n",
      "50/50 [==============================] - 34s 675ms/step - loss: 1.4736 - accuracy: 0.5639 - val_loss: 1.3914 - val_accuracy: 0.5898\n",
      "Epoch 64/150\n",
      "50/50 [==============================] - 34s 668ms/step - loss: 1.4231 - accuracy: 0.5778 - val_loss: 1.3977 - val_accuracy: 0.5859\n",
      "Epoch 65/150\n",
      "50/50 [==============================] - 34s 673ms/step - loss: 1.4259 - accuracy: 0.5741 - val_loss: 1.2161 - val_accuracy: 0.5977\n",
      "Epoch 66/150\n",
      "50/50 [==============================] - 33s 665ms/step - loss: 1.4372 - accuracy: 0.5660 - val_loss: 1.4769 - val_accuracy: 0.5625\n",
      "Epoch 67/150\n",
      "50/50 [==============================] - 35s 695ms/step - loss: 1.4624 - accuracy: 0.5673 - val_loss: 1.3535 - val_accuracy: 0.5625\n",
      "Epoch 68/150\n",
      "50/50 [==============================] - 34s 673ms/step - loss: 1.4204 - accuracy: 0.5734 - val_loss: 1.2869 - val_accuracy: 0.5781\n",
      "Epoch 69/150\n",
      "50/50 [==============================] - 34s 674ms/step - loss: 1.4204 - accuracy: 0.5737 - val_loss: 1.3560 - val_accuracy: 0.5938\n",
      "Epoch 70/150\n",
      "50/50 [==============================] - 33s 666ms/step - loss: 1.3804 - accuracy: 0.5863 - val_loss: 1.2973 - val_accuracy: 0.5586\n",
      "Epoch 71/150\n",
      "50/50 [==============================] - 35s 694ms/step - loss: 1.4092 - accuracy: 0.5762 - val_loss: 1.4446 - val_accuracy: 0.6055\n",
      "Epoch 72/150\n",
      "50/50 [==============================] - 34s 679ms/step - loss: 1.3857 - accuracy: 0.5808 - val_loss: 1.3113 - val_accuracy: 0.6016\n",
      "Epoch 73/150\n",
      "50/50 [==============================] - 35s 698ms/step - loss: 1.3537 - accuracy: 0.5865 - val_loss: 1.3318 - val_accuracy: 0.5938\n",
      "Epoch 74/150\n",
      "50/50 [==============================] - 36s 719ms/step - loss: 1.3692 - accuracy: 0.5848 - val_loss: 1.4208 - val_accuracy: 0.5742\n",
      "Epoch 75/150\n",
      "50/50 [==============================] - 36s 723ms/step - loss: 1.3706 - accuracy: 0.5870 - val_loss: 1.3562 - val_accuracy: 0.5664\n",
      "Epoch 76/150\n",
      "50/50 [==============================] - 37s 747ms/step - loss: 1.3643 - accuracy: 0.5830 - val_loss: 1.3796 - val_accuracy: 0.5977\n",
      "Epoch 77/150\n",
      "50/50 [==============================] - 33s 650ms/step - loss: 1.3130 - accuracy: 0.5981 - val_loss: 1.2923 - val_accuracy: 0.5898\n",
      "Epoch 78/150\n",
      "50/50 [==============================] - 33s 653ms/step - loss: 1.3624 - accuracy: 0.5841 - val_loss: 1.2797 - val_accuracy: 0.6289\n",
      "Epoch 79/150\n",
      "50/50 [==============================] - 35s 690ms/step - loss: 1.3231 - accuracy: 0.5938 - val_loss: 1.2714 - val_accuracy: 0.6094\n",
      "Epoch 80/150\n",
      "50/50 [==============================] - 34s 684ms/step - loss: 1.3200 - accuracy: 0.5986 - val_loss: 1.4325 - val_accuracy: 0.5430\n",
      "Epoch 81/150\n",
      "50/50 [==============================] - 32s 633ms/step - loss: 1.2838 - accuracy: 0.5980 - val_loss: 1.4069 - val_accuracy: 0.6172\n",
      "Epoch 82/150\n",
      "50/50 [==============================] - 33s 649ms/step - loss: 1.3385 - accuracy: 0.5979 - val_loss: 1.4520 - val_accuracy: 0.5508\n",
      "Epoch 83/150\n",
      "50/50 [==============================] - 33s 648ms/step - loss: 1.3203 - accuracy: 0.5954 - val_loss: 1.4113 - val_accuracy: 0.5820\n",
      "Epoch 84/150\n",
      "50/50 [==============================] - 35s 706ms/step - loss: 1.2852 - accuracy: 0.6106 - val_loss: 1.2609 - val_accuracy: 0.6055\n",
      "Epoch 85/150\n",
      "50/50 [==============================] - 34s 670ms/step - loss: 1.2956 - accuracy: 0.6072 - val_loss: 1.3298 - val_accuracy: 0.5508\n",
      "Epoch 86/150\n",
      "50/50 [==============================] - 34s 683ms/step - loss: 1.2955 - accuracy: 0.6051 - val_loss: 1.2259 - val_accuracy: 0.6328\n",
      "Epoch 87/150\n",
      "50/50 [==============================] - 33s 667ms/step - loss: 1.3208 - accuracy: 0.6007 - val_loss: 1.2576 - val_accuracy: 0.5859\n",
      "Epoch 88/150\n",
      "50/50 [==============================] - 33s 668ms/step - loss: 1.2731 - accuracy: 0.6119 - val_loss: 1.0899 - val_accuracy: 0.6211\n",
      "Epoch 89/150\n",
      "50/50 [==============================] - 34s 676ms/step - loss: 1.2301 - accuracy: 0.6214 - val_loss: 1.4743 - val_accuracy: 0.5469\n",
      "Epoch 90/150\n",
      "50/50 [==============================] - 37s 736ms/step - loss: 1.2405 - accuracy: 0.6230 - val_loss: 1.2048 - val_accuracy: 0.6289\n",
      "Epoch 91/150\n",
      "50/50 [==============================] - 33s 666ms/step - loss: 1.2532 - accuracy: 0.6191 - val_loss: 1.2073 - val_accuracy: 0.6289\n",
      "Epoch 92/150\n",
      "50/50 [==============================] - 32s 648ms/step - loss: 1.2151 - accuracy: 0.6263 - val_loss: 1.3528 - val_accuracy: 0.6172\n",
      "Epoch 93/150\n",
      "50/50 [==============================] - 35s 688ms/step - loss: 1.2257 - accuracy: 0.6230 - val_loss: 1.3856 - val_accuracy: 0.5625\n",
      "Epoch 94/150\n",
      "50/50 [==============================] - 33s 659ms/step - loss: 1.2319 - accuracy: 0.6231 - val_loss: 1.2126 - val_accuracy: 0.6523\n",
      "Epoch 95/150\n",
      "50/50 [==============================] - 33s 654ms/step - loss: 1.2304 - accuracy: 0.6195 - val_loss: 1.3006 - val_accuracy: 0.6367\n",
      "Epoch 96/150\n",
      "50/50 [==============================] - 36s 715ms/step - loss: 1.2318 - accuracy: 0.6260 - val_loss: 1.2453 - val_accuracy: 0.5781\n",
      "Epoch 97/150\n",
      "50/50 [==============================] - 34s 681ms/step - loss: 1.2116 - accuracy: 0.6255 - val_loss: 1.4030 - val_accuracy: 0.6016\n",
      "Epoch 98/150\n",
      "50/50 [==============================] - 34s 674ms/step - loss: 1.1821 - accuracy: 0.6316 - val_loss: 1.2458 - val_accuracy: 0.6289\n",
      "Epoch 99/150\n",
      "50/50 [==============================] - 33s 648ms/step - loss: 1.2044 - accuracy: 0.6279 - val_loss: 1.2027 - val_accuracy: 0.5898\n",
      "Epoch 100/150\n",
      "50/50 [==============================] - 33s 661ms/step - loss: 1.2193 - accuracy: 0.6259 - val_loss: 1.3490 - val_accuracy: 0.5859\n",
      "Epoch 101/150\n",
      "50/50 [==============================] - 35s 689ms/step - loss: 1.2079 - accuracy: 0.6324 - val_loss: 1.3588 - val_accuracy: 0.6172\n",
      "Epoch 102/150\n",
      "50/50 [==============================] - 44s 873ms/step - loss: 1.1948 - accuracy: 0.6293 - val_loss: 1.0976 - val_accuracy: 0.6641\n",
      "Epoch 103/150\n",
      "50/50 [==============================] - 35s 682ms/step - loss: 1.1856 - accuracy: 0.6345 - val_loss: 1.1853 - val_accuracy: 0.6562\n",
      "Epoch 104/150\n",
      "50/50 [==============================] - 33s 657ms/step - loss: 1.1710 - accuracy: 0.6392 - val_loss: 1.0878 - val_accuracy: 0.6875\n",
      "Epoch 105/150\n",
      "50/50 [==============================] - 36s 720ms/step - loss: 1.1555 - accuracy: 0.6462 - val_loss: 1.2092 - val_accuracy: 0.6211\n",
      "Epoch 106/150\n",
      "50/50 [==============================] - 35s 693ms/step - loss: 1.1487 - accuracy: 0.6442 - val_loss: 1.0565 - val_accuracy: 0.6328\n",
      "Epoch 107/150\n",
      "50/50 [==============================] - 33s 654ms/step - loss: 1.1505 - accuracy: 0.6402 - val_loss: 1.2751 - val_accuracy: 0.6211\n",
      "Epoch 108/150\n",
      "50/50 [==============================] - 33s 649ms/step - loss: 1.1969 - accuracy: 0.6338 - val_loss: 1.1622 - val_accuracy: 0.6562\n",
      "Epoch 109/150\n",
      "50/50 [==============================] - 33s 650ms/step - loss: 1.1487 - accuracy: 0.6413 - val_loss: 1.2854 - val_accuracy: 0.5938\n",
      "Epoch 110/150\n",
      "50/50 [==============================] - 34s 673ms/step - loss: 1.1623 - accuracy: 0.6423 - val_loss: 1.2382 - val_accuracy: 0.5859\n",
      "Epoch 111/150\n",
      "50/50 [==============================] - 33s 656ms/step - loss: 1.1366 - accuracy: 0.6479 - val_loss: 1.1522 - val_accuracy: 0.6367\n",
      "Epoch 112/150\n",
      "50/50 [==============================] - 33s 657ms/step - loss: 1.1692 - accuracy: 0.6305 - val_loss: 1.1125 - val_accuracy: 0.6562\n",
      "Epoch 113/150\n",
      "50/50 [==============================] - 33s 653ms/step - loss: 1.1285 - accuracy: 0.6503 - val_loss: 0.9406 - val_accuracy: 0.6875\n",
      "Epoch 114/150\n",
      "50/50 [==============================] - 40s 796ms/step - loss: 1.1623 - accuracy: 0.6437 - val_loss: 1.0400 - val_accuracy: 0.6641\n",
      "Epoch 115/150\n",
      "50/50 [==============================] - 34s 676ms/step - loss: 1.1387 - accuracy: 0.6459 - val_loss: 1.2792 - val_accuracy: 0.6484\n",
      "Epoch 116/150\n",
      "50/50 [==============================] - 34s 674ms/step - loss: 1.1517 - accuracy: 0.6437 - val_loss: 1.0279 - val_accuracy: 0.6602\n",
      "Epoch 117/150\n",
      "50/50 [==============================] - 34s 672ms/step - loss: 1.0981 - accuracy: 0.6565 - val_loss: 0.9583 - val_accuracy: 0.6719\n",
      "Epoch 118/150\n",
      "50/50 [==============================] - 35s 690ms/step - loss: 1.1325 - accuracy: 0.6498 - val_loss: 1.1838 - val_accuracy: 0.6289\n",
      "Epoch 119/150\n",
      "50/50 [==============================] - 37s 731ms/step - loss: 1.1239 - accuracy: 0.6512 - val_loss: 1.1430 - val_accuracy: 0.6289\n",
      "Epoch 120/150\n",
      "50/50 [==============================] - 33s 659ms/step - loss: 1.0976 - accuracy: 0.6573 - val_loss: 1.0452 - val_accuracy: 0.6602\n",
      "Epoch 121/150\n",
      "50/50 [==============================] - 33s 665ms/step - loss: 1.1068 - accuracy: 0.6537 - val_loss: 1.1394 - val_accuracy: 0.6758\n",
      "Epoch 122/150\n",
      "50/50 [==============================] - 39s 786ms/step - loss: 1.1190 - accuracy: 0.6470 - val_loss: 1.1493 - val_accuracy: 0.6367\n",
      "Epoch 123/150\n",
      "50/50 [==============================] - 36s 724ms/step - loss: 1.0989 - accuracy: 0.6616 - val_loss: 1.1023 - val_accuracy: 0.6328\n",
      "Epoch 124/150\n",
      "50/50 [==============================] - 33s 661ms/step - loss: 1.0959 - accuracy: 0.6584 - val_loss: 0.9927 - val_accuracy: 0.6680\n",
      "Epoch 125/150\n",
      "50/50 [==============================] - 33s 657ms/step - loss: 1.0864 - accuracy: 0.6589 - val_loss: 1.0135 - val_accuracy: 0.7070\n",
      "Epoch 126/150\n",
      "50/50 [==============================] - 34s 672ms/step - loss: 1.0701 - accuracy: 0.6611 - val_loss: 1.1724 - val_accuracy: 0.6602\n",
      "Epoch 127/150\n",
      "50/50 [==============================] - 34s 674ms/step - loss: 1.0716 - accuracy: 0.6647 - val_loss: 1.0947 - val_accuracy: 0.6484\n",
      "Epoch 128/150\n",
      "50/50 [==============================] - 35s 696ms/step - loss: 1.1010 - accuracy: 0.6556 - val_loss: 1.0930 - val_accuracy: 0.6562\n",
      "Epoch 129/150\n",
      "50/50 [==============================] - 33s 656ms/step - loss: 1.0766 - accuracy: 0.6594 - val_loss: 1.1094 - val_accuracy: 0.6602\n",
      "Epoch 130/150\n",
      "50/50 [==============================] - 34s 680ms/step - loss: 1.0691 - accuracy: 0.6655 - val_loss: 0.9759 - val_accuracy: 0.6562\n",
      "Epoch 131/150\n",
      "50/50 [==============================] - 33s 666ms/step - loss: 1.0926 - accuracy: 0.6624 - val_loss: 1.1016 - val_accuracy: 0.6836\n",
      "Epoch 132/150\n",
      "50/50 [==============================] - 34s 667ms/step - loss: 1.0822 - accuracy: 0.6657 - val_loss: 0.9986 - val_accuracy: 0.6875\n",
      "Epoch 133/150\n",
      "50/50 [==============================] - 34s 674ms/step - loss: 1.1161 - accuracy: 0.6509 - val_loss: 0.9600 - val_accuracy: 0.6992\n",
      "Epoch 134/150\n",
      "50/50 [==============================] - 34s 669ms/step - loss: 1.0673 - accuracy: 0.6657 - val_loss: 1.0970 - val_accuracy: 0.6250\n",
      "Epoch 135/150\n",
      "50/50 [==============================] - 34s 667ms/step - loss: 1.0881 - accuracy: 0.6591 - val_loss: 1.0297 - val_accuracy: 0.6523\n",
      "Epoch 136/150\n",
      "50/50 [==============================] - 33s 666ms/step - loss: 1.0461 - accuracy: 0.6693 - val_loss: 1.1813 - val_accuracy: 0.6094\n",
      "Epoch 137/150\n",
      "50/50 [==============================] - 36s 714ms/step - loss: 1.0399 - accuracy: 0.6724 - val_loss: 0.8904 - val_accuracy: 0.6953\n",
      "Epoch 138/150\n",
      "50/50 [==============================] - 34s 676ms/step - loss: 1.0252 - accuracy: 0.6725 - val_loss: 1.0966 - val_accuracy: 0.6406\n",
      "Epoch 139/150\n",
      "50/50 [==============================] - 36s 708ms/step - loss: 1.0675 - accuracy: 0.6648 - val_loss: 1.0662 - val_accuracy: 0.6680\n",
      "Epoch 140/150\n",
      "50/50 [==============================] - 35s 686ms/step - loss: 1.0334 - accuracy: 0.6761 - val_loss: 1.0868 - val_accuracy: 0.6758\n",
      "Epoch 141/150\n",
      "50/50 [==============================] - 34s 675ms/step - loss: 1.0224 - accuracy: 0.6791 - val_loss: 1.0025 - val_accuracy: 0.6719\n",
      "Epoch 142/150\n",
      "50/50 [==============================] - 34s 677ms/step - loss: 1.0571 - accuracy: 0.6690 - val_loss: 1.0199 - val_accuracy: 0.6680\n",
      "Epoch 143/150\n",
      "50/50 [==============================] - 34s 671ms/step - loss: 1.0373 - accuracy: 0.6703 - val_loss: 1.0428 - val_accuracy: 0.6484\n",
      "Epoch 144/150\n",
      "50/50 [==============================] - 34s 668ms/step - loss: 1.0265 - accuracy: 0.6730 - val_loss: 1.1933 - val_accuracy: 0.6484\n",
      "Epoch 145/150\n",
      "50/50 [==============================] - 36s 709ms/step - loss: 1.0345 - accuracy: 0.6727 - val_loss: 1.2676 - val_accuracy: 0.6055\n",
      "Epoch 146/150\n",
      "50/50 [==============================] - 34s 682ms/step - loss: 1.0347 - accuracy: 0.6765 - val_loss: 1.0968 - val_accuracy: 0.6836\n",
      "Epoch 147/150\n",
      "50/50 [==============================] - 32s 644ms/step - loss: 1.0677 - accuracy: 0.6620 - val_loss: 1.1408 - val_accuracy: 0.6445\n",
      "Epoch 148/150\n",
      "50/50 [==============================] - 33s 654ms/step - loss: 1.0261 - accuracy: 0.6774 - val_loss: 0.9486 - val_accuracy: 0.6836\n",
      "Epoch 149/150\n",
      "50/50 [==============================] - 33s 646ms/step - loss: 1.0470 - accuracy: 0.6724 - val_loss: 0.9797 - val_accuracy: 0.6875\n",
      "Epoch 150/150\n",
      "50/50 [==============================] - 33s 663ms/step - loss: 1.0245 - accuracy: 0.6742 - val_loss: 1.3230 - val_accuracy: 0.6016\n"
     ]
    }
   ],
   "source": [
    "history3 = modelCNN3.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=50,\n",
    "    validation_steps=1,\n",
    "    epochs=150,\n",
    "    validation_data=validation_generator,\n",
    "    callbacks=[tensorboard_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 47s 30ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      0.76      0.68       246\n",
      "           1       0.78      0.71      0.74       258\n",
      "           2       0.56      0.29      0.38       254\n",
      "           3       0.95      0.92      0.93       249\n",
      "           4       0.75      0.81      0.78       247\n",
      "           5       0.59      0.56      0.57       262\n",
      "           6       0.74      0.53      0.62       255\n",
      "           7       0.52      0.45      0.49       256\n",
      "           8       0.91      0.75      0.82       246\n",
      "           9       0.85      0.67      0.75       248\n",
      "          10       0.66      0.94      0.78       266\n",
      "          11       0.59      0.50      0.54       231\n",
      "          12       0.52      0.66      0.58       240\n",
      "          13       0.82      0.83      0.82       261\n",
      "          14       0.94      0.91      0.93       239\n",
      "          15       0.91      0.80      0.85       253\n",
      "          16       0.89      0.83      0.86       258\n",
      "          17       0.84      0.82      0.83       253\n",
      "          18       0.61      0.59      0.60       257\n",
      "          19       0.72      0.83      0.77       242\n",
      "          20       0.75      0.74      0.75       266\n",
      "          21       0.75      0.76      0.76       258\n",
      "          22       0.65      0.51      0.57       245\n",
      "          23       0.78      0.82      0.80       244\n",
      "          24       0.62      0.70      0.66       249\n",
      "          25       0.78      0.18      0.29       259\n",
      "          26       0.83      0.52      0.64       248\n",
      "          27       0.95      0.95      0.95       241\n",
      "          28       0.63      0.80      0.71       256\n",
      "          29       0.71      0.44      0.54       239\n",
      "          30       0.83      0.82      0.82       239\n",
      "          31       0.66      0.76      0.70       243\n",
      "          32       0.68      0.68      0.68       248\n",
      "          33       0.46      0.57      0.51       246\n",
      "          34       0.91      0.76      0.83       254\n",
      "          35       0.77      0.84      0.80       244\n",
      "          36       0.72      0.73      0.72       249\n",
      "          37       0.90      0.93      0.92       254\n",
      "          38       0.63      0.81      0.71       250\n",
      "          39       0.76      0.67      0.71       247\n",
      "          40       0.49      0.59      0.54       244\n",
      "          41       0.94      0.69      0.79       262\n",
      "          42       0.67      0.91      0.77       243\n",
      "          43       0.90      0.82      0.86       267\n",
      "          44       0.93      0.49      0.64       250\n",
      "          45       0.88      0.92      0.90       244\n",
      "          46       0.53      0.89      0.66       239\n",
      "          47       0.44      0.22      0.29       247\n",
      "          48       0.80      0.65      0.71       262\n",
      "          49       0.77      0.88      0.82       247\n",
      "          50       0.61      0.70      0.66       233\n",
      "          51       0.87      0.82      0.84       235\n",
      "          52       0.73      0.64      0.69       236\n",
      "          53       0.84      0.70      0.76       251\n",
      "          54       0.86      0.56      0.68       240\n",
      "          55       0.36      0.15      0.21       233\n",
      "          56       0.37      0.67      0.48       258\n",
      "          57       0.47      0.56      0.51       237\n",
      "          58       0.49      0.69      0.57       255\n",
      "          59       0.38      0.53      0.44       248\n",
      "          60       0.46      0.69      0.55       248\n",
      "          61       0.59      0.86      0.70       251\n",
      "          62       0.76      0.33      0.46       254\n",
      "          63       0.72      0.49      0.58       241\n",
      "          64       0.61      0.58      0.60       242\n",
      "          65       0.78      0.51      0.61       253\n",
      "          66       0.97      0.94      0.96       266\n",
      "          67       0.84      0.94      0.88       264\n",
      "          68       0.81      0.76      0.79       261\n",
      "          69       0.93      0.84      0.88       237\n",
      "          70       0.72      0.86      0.78       263\n",
      "          71       0.52      0.66      0.58       248\n",
      "          72       0.60      0.84      0.70       236\n",
      "          73       0.66      0.80      0.72       254\n",
      "          74       0.68      0.52      0.59       240\n",
      "          75       0.52      0.26      0.35       239\n",
      "          76       0.71      0.91      0.80       237\n",
      "          77       0.44      0.50      0.46       253\n",
      "          78       0.32      0.65      0.43       256\n",
      "          79       0.41      0.50      0.45       251\n",
      "          80       0.56      0.80      0.66       254\n",
      "          81       0.79      0.91      0.84       234\n",
      "          82       0.59      0.77      0.66       248\n",
      "          83       0.62      0.89      0.74       228\n",
      "          84       0.84      0.69      0.76       259\n",
      "          85       0.88      0.96      0.92       243\n",
      "          86       0.91      0.87      0.89       246\n",
      "          87       0.57      0.37      0.45       247\n",
      "          88       0.80      0.68      0.74       250\n",
      "          89       0.83      0.87      0.85       253\n",
      "          90       0.94      0.91      0.92       267\n",
      "          91       0.54      0.64      0.59       243\n",
      "          92       0.82      0.46      0.59       252\n",
      "          93       0.45      0.45      0.45       234\n",
      "          94       0.76      0.75      0.75       264\n",
      "          95       0.79      0.32      0.45       225\n",
      "          96       0.41      0.48      0.44       250\n",
      "          97       0.93      0.86      0.89       250\n",
      "          98       0.81      0.78      0.80       249\n",
      "          99       0.37      0.74      0.50       254\n",
      "         100       0.83      0.63      0.72       256\n",
      "         101       0.84      0.48      0.61       254\n",
      "         102       0.93      0.81      0.87       269\n",
      "         103       0.47      0.58      0.52       238\n",
      "         104       0.81      0.88      0.85       251\n",
      "         105       0.46      0.63      0.53       231\n",
      "         106       0.59      0.67      0.63       254\n",
      "         107       0.71      0.83      0.77       249\n",
      "         108       0.64      0.49      0.56       257\n",
      "         109       0.54      0.67      0.60       245\n",
      "         110       0.61      0.51      0.56       259\n",
      "         111       0.96      0.78      0.86       244\n",
      "         112       0.93      0.89      0.91       255\n",
      "         113       0.49      0.30      0.37       264\n",
      "         114       0.57      0.68      0.62       257\n",
      "         115       0.91      0.81      0.85       244\n",
      "         116       0.45      0.45      0.45       231\n",
      "         117       0.82      0.77      0.79       254\n",
      "         118       0.71      0.60      0.65       255\n",
      "         119       0.72      0.32      0.44       251\n",
      "         120       0.78      0.77      0.77       260\n",
      "         121       0.96      0.86      0.91       239\n",
      "         122       0.78      0.87      0.82       259\n",
      "         123       0.43      0.45      0.44       249\n",
      "         124       0.56      0.54      0.55       267\n",
      "         125       0.68      0.51      0.58       249\n",
      "         126       0.62      0.76      0.69       258\n",
      "         127       0.81      0.57      0.67       249\n",
      "         128       0.39      0.47      0.43       261\n",
      "         129       0.94      0.89      0.91       243\n",
      "         130       0.81      0.86      0.84       233\n",
      "         131       0.49      0.67      0.57       256\n",
      "         132       0.63      0.23      0.34       245\n",
      "         133       0.85      0.81      0.83       243\n",
      "         134       0.78      0.83      0.81       251\n",
      "         135       0.53      0.63      0.58       266\n",
      "         136       0.37      0.52      0.43       250\n",
      "         137       0.52      0.43      0.47       255\n",
      "         138       0.47      0.59      0.52       242\n",
      "         139       0.47      0.66      0.54       258\n",
      "         140       0.52      0.46      0.49       255\n",
      "         141       0.59      0.69      0.64       249\n",
      "         142       0.73      0.80      0.76       258\n",
      "         143       0.89      0.89      0.89       261\n",
      "         144       0.86      0.75      0.80       257\n",
      "         145       0.70      0.59      0.64       250\n",
      "         146       0.88      0.93      0.90       255\n",
      "         147       0.56      0.52      0.54       248\n",
      "         148       0.97      0.89      0.93       257\n",
      "         149       0.74      0.45      0.56       246\n",
      "         150       0.52      0.52      0.52       245\n",
      "         151       0.89      0.92      0.90       251\n",
      "         152       0.75      0.87      0.80       247\n",
      "         153       0.52      0.36      0.43       273\n",
      "         154       0.65      0.87      0.74       259\n",
      "         155       0.71      0.75      0.73       260\n",
      "         156       0.98      0.96      0.97       256\n",
      "         157       0.88      0.79      0.83       247\n",
      "         158       0.58      0.66      0.62       243\n",
      "         159       0.81      0.64      0.72       235\n",
      "         160       0.71      0.74      0.73       264\n",
      "         161       0.98      0.91      0.94       245\n",
      "         162       0.67      0.86      0.75       251\n",
      "         163       0.75      0.58      0.65       250\n",
      "         164       0.47      0.67      0.55       249\n",
      "         165       0.71      0.69      0.70       255\n",
      "         166       0.96      0.82      0.89       245\n",
      "         167       0.47      0.63      0.54       251\n",
      "         168       0.90      0.63      0.74       240\n",
      "         169       0.62      0.43      0.51       246\n",
      "         170       0.69      0.81      0.74       250\n",
      "         171       0.84      0.77      0.80       252\n",
      "         172       0.92      0.93      0.93       226\n",
      "         173       0.85      0.69      0.76       267\n",
      "         174       0.84      0.47      0.60       249\n",
      "         175       0.70      0.75      0.72       257\n",
      "         176       0.91      0.91      0.91       264\n",
      "         177       0.54      0.40      0.46       255\n",
      "         178       0.49      0.47      0.48       246\n",
      "         179       0.51      0.72      0.60       237\n",
      "         180       0.44      0.39      0.42       263\n",
      "         181       0.76      0.47      0.58       250\n",
      "         182       0.65      0.67      0.66       251\n",
      "         183       0.37      0.13      0.19       252\n",
      "         184       0.55      0.84      0.66       257\n",
      "         185       0.98      0.54      0.70       248\n",
      "         186       0.51      0.82      0.63       249\n",
      "         187       0.64      0.59      0.61       240\n",
      "         188       0.56      0.67      0.61       259\n",
      "         189       0.96      0.98      0.97       253\n",
      "         190       0.41      0.64      0.50       245\n",
      "         191       0.72      0.68      0.70       234\n",
      "         192       0.69      0.54      0.60       261\n",
      "         193       0.87      0.85      0.86       246\n",
      "         194       0.77      0.78      0.77       241\n",
      "         195       0.74      0.84      0.79       266\n",
      "         196       0.58      0.44      0.50       254\n",
      "         197       0.66      0.74      0.70       254\n",
      "         198       0.63      0.47      0.54       252\n",
      "         199       0.64      0.58      0.61       248\n",
      "\n",
      "    accuracy                           0.68     50000\n",
      "   macro avg       0.70      0.68      0.67     50000\n",
      "weighted avg       0.70      0.68      0.67     50000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred3 = modelCNN3.predict(x_test)\n",
    "print(classification_report(y_test.argmax(axis=1), y_pred3.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelCNN3.save(r'C:\\Users\\monop\\Desktop\\BOOTCAMP\\projekt_DL\\model068acc.h5')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
